{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDuprC4oxDbS"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:34:43.152969Z",
     "start_time": "2019-12-09T05:34:38.972190Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O4hspJq8MOKc",
    "outputId": "49195e20-1588-4cf7-d642-7c6b280e48de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Importing pytorch functions and modules\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,mean_squared_error,accuracy_score\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "\n",
    "\n",
    "from memory_embeddings import EmbeddingNNModel\n",
    "#Setting random seed for reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:34:44.193224Z",
     "start_time": "2019-12-09T05:34:43.163354Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "ingEmIqPMOKn",
    "outputId": "952ba3c5-3524-44b8-a54a-d514e4707648"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>divey_False</th>\n",
       "      <th>trendy_False</th>\n",
       "      <th>casual_False</th>\n",
       "      <th>casual_True</th>\n",
       "      <th>garage_False</th>\n",
       "      <th>street_False</th>\n",
       "      <th>lot_False</th>\n",
       "      <th>lot_True</th>\n",
       "      <th>NoiseLevel_NA</th>\n",
       "      <th>NoiseLevel_average</th>\n",
       "      <th>BusinessAcceptsCreditCards_NA</th>\n",
       "      <th>RestaurantsDelivery_False</th>\n",
       "      <th>Alcohol_full_bar</th>\n",
       "      <th>Alcohol_none</th>\n",
       "      <th>WiFi_NA</th>\n",
       "      <th>WiFi_free</th>\n",
       "      <th>WiFi_no</th>\n",
       "      <th>RestaurantsReservations_False</th>\n",
       "      <th>RestaurantsPriceRange2_1</th>\n",
       "      <th>RestaurantsPriceRange2_2</th>\n",
       "      <th>GoodForKids_True</th>\n",
       "      <th>HasTV_False</th>\n",
       "      <th>HasTV_True</th>\n",
       "      <th>OutdoorSeating_False</th>\n",
       "      <th>OutdoorSeating_True</th>\n",
       "      <th>Caters_False</th>\n",
       "      <th>Caters_NA</th>\n",
       "      <th>Caters_True</th>\n",
       "      <th>dessert_False</th>\n",
       "      <th>lunch_False</th>\n",
       "      <th>lunch_True</th>\n",
       "      <th>dinner_False</th>\n",
       "      <th>dinner_True</th>\n",
       "      <th>BikeParking_NA</th>\n",
       "      <th>BikeParking_True</th>\n",
       "      <th>RestaurantsTableService_NA</th>\n",
       "      <th>RestaurantsTableService_True</th>\n",
       "      <th>review_id</th>\n",
       "      <th>business_legacy_stars_mean</th>\n",
       "      <th>business_legacy_funny</th>\n",
       "      <th>business_legacy_cool</th>\n",
       "      <th>business_legacy_useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "      <th>yelping_since_td</th>\n",
       "      <th>user_legacy_stars_mean</th>\n",
       "      <th>user_legacy_funny</th>\n",
       "      <th>user_legacy_cool</th>\n",
       "      <th>user_legacy_useful</th>\n",
       "      <th>stars_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>S1aROCxDQMyG_EYgpB-CAA</td>\n",
       "      <td>0.672935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>UtkjXTiMfGRz4R1zDF3Kxg</td>\n",
       "      <td>0.878213</td>\n",
       "      <td>0.874866</td>\n",
       "      <td>0.975061</td>\n",
       "      <td>0.969967</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.859553</td>\n",
       "      <td>0.853539</td>\n",
       "      <td>0.841861</td>\n",
       "      <td>0.841861</td>\n",
       "      <td>0.830966</td>\n",
       "      <td>0.625867</td>\n",
       "      <td>0.207319</td>\n",
       "      <td>0.592872</td>\n",
       "      <td>0.325596</td>\n",
       "      <td>0.578379</td>\n",
       "      <td>0.484170</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0TDA4X4HK9PjZSz4TM5n5A</td>\n",
       "      <td>0.849047</td>\n",
       "      <td>0.099229</td>\n",
       "      <td>0.157775</td>\n",
       "      <td>0.113951</td>\n",
       "      <td>PuPWzbQJ6P_tax-holDgSQ</td>\n",
       "      <td>0.236881</td>\n",
       "      <td>0.358604</td>\n",
       "      <td>0.602501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.476710</td>\n",
       "      <td>0.181533</td>\n",
       "      <td>0.181533</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322367</td>\n",
       "      <td>0.679776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>M0Y9h_qdU_LF5_TkltRkJQ</td>\n",
       "      <td>0.907751</td>\n",
       "      <td>0.099229</td>\n",
       "      <td>0.157775</td>\n",
       "      <td>0.140668</td>\n",
       "      <td>fEo1C7GJlK8HlfRfNouLbw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331516</td>\n",
       "      <td>0.632618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172937</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>D2oYk9ipsvsYP_ls0aL9kg</td>\n",
       "      <td>0.937103</td>\n",
       "      <td>0.099229</td>\n",
       "      <td>0.157775</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>9IA3P3DxrZcoAl29IYOloA</td>\n",
       "      <td>0.236881</td>\n",
       "      <td>0.645229</td>\n",
       "      <td>0.602501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191458</td>\n",
       "      <td>0.521292</td>\n",
       "      <td>0.478216</td>\n",
       "      <td>0.478216</td>\n",
       "      <td>0.379649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314580</td>\n",
       "      <td>0.813156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397308</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eU_713ec6fTGNO4BegRaww</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4yLl21v8cJEgJSjYCKoV5Q</td>\n",
       "      <td>0.897848</td>\n",
       "      <td>0.099229</td>\n",
       "      <td>0.184420</td>\n",
       "      <td>0.161335</td>\n",
       "      <td>7MI0oX7_fTwjU5NML80jsw</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502679</td>\n",
       "      <td>0.180652</td>\n",
       "      <td>0.436429</td>\n",
       "      <td>0.436429</td>\n",
       "      <td>0.426655</td>\n",
       "      <td>0.571068</td>\n",
       "      <td>0.364804</td>\n",
       "      <td>0.813156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397308</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  divey_False  ...  user_legacy_useful  stars_review\n",
       "0  eU_713ec6fTGNO4BegRaww            1  ...            0.484170             5\n",
       "1  eU_713ec6fTGNO4BegRaww            1  ...            0.000000             5\n",
       "2  eU_713ec6fTGNO4BegRaww            1  ...            0.172937             5\n",
       "3  eU_713ec6fTGNO4BegRaww            1  ...            0.397308             4\n",
       "4  eU_713ec6fTGNO4BegRaww            1  ...            0.397308             5\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = \"task1_processed_data.csv\"\n",
    "total_data = pd.read_csv(input_data)\n",
    "total_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:34:44.197387Z",
     "start_time": "2019-12-09T05:34:44.194670Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fyvRgUcZMOKr"
   },
   "outputs": [],
   "source": [
    "features = [col for col in total_data.columns if col not in [\"review_id\",\"business_id\",\"user_id\",\"stars_review\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:34:44.238281Z",
     "start_time": "2019-12-09T05:34:44.198693Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tPR89gOiMOKu"
   },
   "outputs": [],
   "source": [
    "def nearest_integer(x):\n",
    "  x = max(0,x)\n",
    "  x = min(x,4)\n",
    "  x_int = int(x)\n",
    "  if(x-x_int>=0.5):\n",
    "    return(x_int+1)\n",
    "  else:\n",
    "    return(x_int)\n",
    "\n",
    "\n",
    "\n",
    "class myDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, images,labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return(self.images.shape[0]) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return the augmented image\n",
    "        assert(idx<self.images.shape[0])\n",
    "        return (torch.tensor(self.images[idx,:],dtype=torch.float64),torch.tensor(self.labels.reshape(-1)[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMDvPAF1xKvN"
   },
   "source": [
    "# Defining neural network and training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:34:44.249673Z",
     "start_time": "2019-12-09T05:34:44.239451Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rlnQ9gtKMOKx"
   },
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self,input_dm,output_dm,a=-2,dropout=0):\n",
    "        super(Neural_Network,self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dm, 64)\n",
    "        self.hidden_layer_1 = nn.Linear(64, 32)\n",
    "        self.hidden_layer_2 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16,output_dm)\n",
    "        self.act = torch.nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=input_dm,affine=False,track_running_stats=False)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=input_dm,affine=False,track_running_stats=False)\n",
    "        self.dd = nn.Dropout(dropout)\n",
    "#         print(\"Neural network defined for device: \"+str(device))\n",
    "#         print(\"Input image shape: [\"+str(input_dm1)+\",\"+str(input_dm2)+\"]\")\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dd(x)\n",
    "        \n",
    "        #hidden layers\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dd(x)\n",
    "        x = self.hidden_layer_2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dd(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.dd(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T05:39:06.253471Z",
     "start_time": "2019-12-09T05:39:06.226210Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tTk1X8UFMOK0"
   },
   "outputs": [],
   "source": [
    "class train_neural_network:\n",
    "\n",
    "    def __init__(self,Model_Class,batch_size,total_data,features,target,\n",
    "                 stratify_var,user_embedding_size,business_embedding_size,device,use_saved_loaders):\n",
    "        \n",
    "        \n",
    "        super(train_neural_network,self).__init__()\n",
    "\n",
    "        labels = total_data[target].values - 1\n",
    "        if(use_saved_loaders==True):\n",
    "          try:\n",
    "            self.train_loader = torch.load(\"temp_train_loader.pt\")\n",
    "            self.val_loader = torch.load(\"temp_val_loader.pt\")\n",
    "            self.n_train = np.sum(np.array([x.shape[0] for x,y in self.train_loader]))\n",
    "            self.n_val = np.sum(np.array([x.shape[0] for x,y in self.val_loader]))\n",
    "          except:\n",
    "            print(\"Data loaders not found, creating instead\")\n",
    "            use_saved_loaders=False\n",
    "        if(use_saved_loaders==False):\n",
    "          emb = EmbeddingNNModel(total_data,user_embedding_size,business_embedding_size,\"model2\")\n",
    "          self.batch_size = batch_size\n",
    "\n",
    "          self.train_df,self.val_df,_,_ = train_test_split(total_data,total_data[target].values,\n",
    "                                                stratify=total_data[stratify_var].values,\n",
    "                                                        shuffle=True,random_state=SEED)\n",
    "          emb.model_training(self.train_df,SEED)\n",
    "          \n",
    "          x_train_u = emb.get_user_embeddings(self.train_df[\"user_id\"].values)\n",
    "          x_train_b = emb.get_business_embeddings(self.train_df[\"business_id\"].values)\n",
    "          self.x_train = np.concatenate((self.train_df[features].values,x_train_u,x_train_b),axis=1)\n",
    "          self.n_train = self.x_train.shape[0]\n",
    "          self.y_train = self.train_df[target].values - 1 \n",
    "          \n",
    "          x_val_u = emb.get_user_embeddings(self.val_df[\"user_id\"].values)\n",
    "          x_val_b = emb.get_business_embeddings(self.val_df[\"business_id\"].values)\n",
    "          self.x_val = np.concatenate((self.val_df[features].values,x_val_u,x_val_b),axis=1)\n",
    "          self.n_val = self.x_val.shape[0]\n",
    "          self.y_val = self.val_df[target].values - 1 \n",
    "          self.n_val = self.x_val.shape[0]\n",
    "          \n",
    "          train_dataset = myDataset(self.x_train,self.y_train)\n",
    "          val_dataset = myDataset(self.x_val,self.y_val)\n",
    "          \n",
    "          \n",
    "          weights_train = self.make_weights_for_balanced_classes(labels,self.y_train)\n",
    "          weights_train = torch.DoubleTensor(weights_train)\n",
    "          sampler_train = torch.utils.data.sampler.WeightedRandomSampler(weights_train, len(weights_train))\n",
    "          self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size,shuffle=True)                              \n",
    "                                                      #sampler = sampler_train, num_workers=0)\n",
    "          \n",
    "          weights_val = self.make_weights_for_balanced_classes(labels,self.y_val)\n",
    "          weights_val = torch.DoubleTensor(weights_val)\n",
    "          sampler_val = torch.utils.data.sampler.WeightedRandomSampler(weights_val, len(weights_val))\n",
    "          self.val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(self.y_val),shuffle=True)                             \n",
    "                                                      #sampler = sampler_val, num_workers=0)\n",
    "          torch.save(self.train_loader,\"temp_train_loader.pt\")\n",
    "          torch.save(self.val_loader,\"temp_val_loader.pt\")\n",
    "        \n",
    "        x_sample,y_sample = next(iter(self.train_loader))\n",
    "        self.input_dm = x_sample.shape[1]\n",
    "        self.cw = self.make_class_weights(labels)\n",
    "        self.Model_Class = Model_Class\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self,max_epochs,verbose,learning_rate,class_weight,dropout=0):\n",
    "        self.dropout = dropout\n",
    "        self.model = self.Model_Class(input_dm = self.input_dm,\n",
    "                                    output_dm = 5,\n",
    "                                    dropout=self.dropout).to(device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        if(class_weight==\"uniform\"):\n",
    "          cw = torch.ones((5))\n",
    "        else:\n",
    "          cw = self.cw\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=cw).to(device)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "        epoch = 0\n",
    "        rounds = 0\n",
    "        early_stopping_rounds = 5\n",
    "        stop = False\n",
    "        \n",
    "\n",
    "        while ((epoch < max_epochs)&(stop==False)):\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for x,y in self.train_loader:\n",
    "                y = torch.tensor(y).to(device)\n",
    "                x = x.float().to(device)\n",
    "                y_pred_hat = F.softmax(self.model.forward(x),dim=1)\n",
    "\n",
    "                batch_ce_loss = criterion(y_pred_hat,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_ce_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss = total_loss + batch_ce_loss.item()*x.shape[0]\n",
    "            train_loss = total_loss/self.n_train\n",
    "            self.train_loss_history.append(train_loss)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            x_val,y_val =  next(iter(self.val_loader))\n",
    "            y_val = torch.tensor(y_val).to(device)\n",
    "            x_val = x_val.float().to(device)\n",
    "            y_pred_hat = F.softmax(self.model.forward(x_val),dim=1)\n",
    "            y_pred = torch.argmax(y_pred_hat,1)\n",
    "            f1 = f1_score(y_val.cpu(),y_pred.cpu(),average=\"micro\")\n",
    "            acc = accuracy_score(y_val.cpu(),y_pred.cpu())\n",
    "            mse = mean_squared_error(y_val.cpu(),y_pred.cpu())\n",
    "            val_loss = round(criterion(y_pred_hat,y_val).item(),2)\n",
    "\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "\n",
    "            if(val_loss<self.best_loss):\n",
    "                self.best_loss = val_loss\n",
    "                rounds = 0\n",
    "            else:\n",
    "                rounds += 1\n",
    "\n",
    "\n",
    "            if(rounds>=early_stopping_rounds):\n",
    "                stop=True\n",
    "                \n",
    "            \n",
    "            if((verbose==True)&((epoch%5==0)|(stop==True))):\n",
    "                print(\"----------------------------------------------------------------------------------\")\n",
    "                print(\"EPOCH:\"+str(epoch))\n",
    "                if(stop==True):\n",
    "                    print(\"Training to be concluded after this epoch\") \n",
    "                print(\"Average training loss  = \"+str(train_loss))\n",
    "                print(\"Average Validation loss  = \"+str(val_loss))\n",
    "                print(\"Micro F1 Score  = \"+str(f1))\n",
    "                #print(\"Accuracy  = \"+str(acc))\n",
    "                print(\"MSE Score  = \"+str(mse))\n",
    "\n",
    "\n",
    "\n",
    "            epoch += 1\n",
    "        #While loop ends\n",
    "                        \n",
    "        self.model_state = self.model.state_dict()\n",
    "        \n",
    "        self.plot_loss()       \n",
    "\n",
    "    def train_for_mse(self,max_epochs,verbose,learning_rate,dropout):\n",
    "        \n",
    "        self.dropout=dropout\n",
    "        self.model = self.Model_Class(input_dm = self.input_dm,\n",
    "                                    output_dm = 1,\n",
    "                                    dropout=self.dropout).to(device)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        criterion = torch.nn.MSELoss().to(device)\n",
    "        ht = torch.nn.Hardtanh(min_val=0.0, max_val=4.0)\n",
    "\n",
    "        self.train_loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.best_loss = np.inf\n",
    "        \n",
    "        epoch = 0\n",
    "        rounds = 0\n",
    "        early_stopping_rounds = 5\n",
    "        stop = False\n",
    "        \n",
    "\n",
    "        while ((epoch < max_epochs)&(stop==False)):\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for x,y in self.train_loader:\n",
    "                y = torch.tensor(y,dtype=torch.float,requires_grad=True).to(device)\n",
    "                x = x.float().to(device)\n",
    "                y_pred = torch.tensor(self.model.forward(x),requires_grad=True,dtype=torch.float).to(device).reshape(-1)\n",
    "                y_pred = 4*F.sigmoid(y_pred)\n",
    "                batch_mse_loss = criterion(y_pred,y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                batch_mse_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss = total_loss + batch_ce_loss.item()*x.shape[0]\n",
    "            train_loss = total_loss/self.n_train\n",
    "            self.train_loss_history.append(train_loss)\n",
    "\n",
    "            total_loss = 0.0\n",
    "            x_val,y_val =  next(iter(self.val_loader))\n",
    "            y_val = torch.tensor(y_val,dtype=torch.float,requires_grad=False).to(device)\n",
    "            x_val = x_val.float().to(device)\n",
    "            y_pred = torch.tensor(self.model.forward(x_val),requires_grad=False,dtype=torch.float).to(device).reshape(-1)\n",
    "            y_pred = 4*F.sigmoid(y_pred)\n",
    "            \n",
    "            \n",
    "            mse = mean_squared_error(y_val.cpu(),y_pred.cpu())\n",
    "            val_loss = round(criterion(y_pred,y_val).item(),2)\n",
    "            ne = np.vectorize(nearest_integer)\n",
    "            y_pred_int = ne(y_pred.detach().cpu().numpy())\n",
    "            y_val_int = y_val.cpu().numpy()\n",
    "            f1 = f1_score(y_val_int,y_pred_int,average=\"micro\")\n",
    "            acc = accuracy_score(y_val_int,y_pred_int)\n",
    "\n",
    "\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "\n",
    "            if(val_loss<self.best_loss):\n",
    "                self.best_loss = val_loss\n",
    "                rounds = 0\n",
    "            else:\n",
    "                rounds += 1\n",
    "\n",
    "\n",
    "            if(rounds>=early_stopping_rounds):\n",
    "                stop=True\n",
    "                \n",
    "            \n",
    "            if((verbose==True)&((epoch%5==0)|(stop==True))):\n",
    "                print(\"----------------------------------------------------------------------------------\")\n",
    "                print(\"EPOCH:\"+str(epoch))\n",
    "                if(stop==True):\n",
    "                    print(\"Training to be concluded after this epoch\") \n",
    "                print(\"Average training loss  = \"+str(train_loss))\n",
    "                print(\"Average Validation loss  = \"+str(val_loss))\n",
    "                print(\"MSE Score  = \"+str(mse))\n",
    "                print(\"Micro F1 Score  = \"+str(f1))\n",
    "\n",
    "\n",
    "\n",
    "            epoch += 1\n",
    "        #While loop ends\n",
    "                        \n",
    "        self.model_state = self.model.state_dict()\n",
    "        \n",
    "        self.plot_loss()       \n",
    "            \n",
    "        \n",
    "    def plot_loss(self):\n",
    "        print(\"----------------------------------------------------------------------------------\")\n",
    "        plt.plot(self.train_loss_history)\n",
    "        plt.plot(self.val_loss_history)\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "    def make_weights_for_balanced_classes(self,labels_total,labels_curr):\n",
    "        classes,counts = np.unique(labels_total,return_counts=True)\n",
    "        counts = np.divide(np.sum(counts),counts)      \n",
    "        class_weights = dict(zip(classes,counts))\n",
    "        weights = np.apply_along_axis(lambda x:class_weights[x[0]],1,labels_curr.reshape(-1,1))\n",
    "        weights_sum = np.sum(weights)\n",
    "        weights = weights / (weights_sum/len(weights))\n",
    "        return(weights)\n",
    "\n",
    "    def make_class_weights(self,labels):\n",
    "        cw = np.zeros((len(np.unique(labels))))\n",
    "        for i in range(0,len(cw)):\n",
    "          cw[i] = np.sum(labels==i)\n",
    "        cw = np.divide(len(labels),cw)\n",
    "        return(torch.tensor(cw,dtype=torch.float))\n",
    "                                      \n",
    "    #def predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huKSI8IqxSdx"
   },
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-09T05:44:36.646Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ynfgmYj_MOK3"
   },
   "outputs": [],
   "source": [
    "tr = train_neural_network(Model_Class=Neural_Network,\n",
    "                          batch_size=5000,\n",
    "                          total_data=total_data,\n",
    "                          features=[],\n",
    "                          target=\"stars_review\",\n",
    "                          stratify_var=\"user_id\",\n",
    "                          user_embedding_size=40,\n",
    "                          business_embedding_size=40,\n",
    "                          device=device,\n",
    "                          use_saved_loaders=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-09T05:44:37.093Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UdvoPugzMOK7"
   },
   "outputs": [],
   "source": [
    "tr.train(max_epochs=200,verbose=True,learning_rate=0.0005,dropout=0,class_weight='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "colab_type": "code",
    "id": "OwKklYWKg_Bj",
    "outputId": "5843f602-728e-4a0a-b10f-acb287284225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------\n",
      "EPOCH:0\n",
      "Average training loss  = 1.4332624028002472\n",
      "Average Validation loss  = 1.41\n",
      "MSE Score  = 1.4149091\n",
      "Micro F1 Score  = 0.34230223570275425\n",
      "----------------------------------------------------------------------------------\n",
      "EPOCH:5\n",
      "Training to be concluded after this epoch\n",
      "Average training loss  = 1.4332624328227226\n",
      "Average Validation loss  = 1.41\n",
      "MSE Score  = 1.4149091\n",
      "Micro F1 Score  = 0.34230223570275425\n",
      "----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkloxOCRJrfx/NPyBCc8koIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tr.train_for_mse(max_epochs=200,verbose=True,learning_rate=0.0001,dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqbNRfD3xYGL"
   },
   "source": [
    "# Trying LightGBM cause why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "azmemwPu29A4",
    "outputId": "b68544b4-38d9-4c3c-b6ab-82f8e6a2f575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.17.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lightgbm) (0.14.1)\n",
      "Collecting bayesian-optimization\n",
      "  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.1)\n",
      "Building wheels for collected packages: bayesian-optimization\n",
      "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=d485fefdab767a9de2583926a99b5f4c82748a7c3494705660e6a00fdf34d455\n",
      "  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n",
      "Successfully built bayesian-optimization\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n",
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xm2hF0Nr_DQv"
   },
   "outputs": [],
   "source": [
    "# x_train = tr.train_df[features].values\n",
    "# y_train = tr.train_df[\"stars_review\"].values\n",
    "# x_val = tr.val_df[features].values\n",
    "# y_val = tr.val_df[\"stars_review\"].values\n",
    "\n",
    "x_train = tr.x_train\n",
    "x_val = tr.x_val\n",
    "y_train = tr.y_train\n",
    "y_val = tr.y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcxtOw-d-GnG"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier,LGBMRegressor\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6ockWq_-lX7"
   },
   "outputs": [],
   "source": [
    "def bayes_opt_f1(x_train,y_train,x_val,y_val, init_round=15, opt_round=15, random_seed=SEED):\n",
    "    t1 = time.time()\n",
    "    \n",
    "\n",
    "        \n",
    "    def lgb_eval(learning_rate,n_estimators,max_depth,min_child_weight,gamma,subsample,colsample_bytree,reg_alpha):\n",
    "        \n",
    "        lgc = LGBMClassifier(learning_rate=learning_rate,n_estimators=int(n_estimators)\n",
    "                            ,max_depth=int(max_depth),min_child_weight=min_child_weight,\n",
    "                             gamma=gamma,subsample=subsample,colsample_bytree=colsample_bytree,\n",
    "                             reg_aplha=reg_alpha)\n",
    "        lgc.fit(x_train,y_train)\n",
    "        y_val_pred = lgc.predict(x_val)\n",
    "        return(f1_score(y_val,y_val_pred,average='micro'))\n",
    "        \n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01,0.2),\n",
    "                                            'n_estimators': (10,100),\n",
    "                                            'max_depth': (5, 8.99),\n",
    "                                            'min_child_weight': (5, 50),\n",
    "                                            'gamma':(0,0.5),\n",
    "                                            'subsample': (0.4, 1.0),\n",
    "                                            'colsample_bytree' :(0.4, 1.0),\n",
    "                                            'reg_alpha':(0.001,100)\n",
    "                                            }, \n",
    "                                               random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "\n",
    "    \n",
    "    # return best parameters\n",
    "    targets = np.array([d['target'] for d in lgbBO.res])\n",
    "    opt_params = lgbBO.res[np.argmax(targets)]['params']\n",
    "    opt_params['max_depth'] = int(opt_params['max_depth'])\n",
    "    opt_params['n_estimators'] = int(opt_params['n_estimators'])\n",
    "    lgc = LGBMClassifier(**opt_params)\n",
    "    print(\"Time taken for hyper-parameter optimization:\" + str(time.time()-t1) )\n",
    "    return(lgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "0CKgWfap_bby",
    "outputId": "444250c8-d05e-416a-c154-bd8bb12ff673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | reg_alpha | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.4222  \u001b[0m | \u001b[0m 0.7293  \u001b[0m | \u001b[0m 0.3576  \u001b[0m | \u001b[0m 0.1245  \u001b[0m | \u001b[0m 7.174   \u001b[0m | \u001b[0m 24.06   \u001b[0m | \u001b[0m 68.13   \u001b[0m | \u001b[0m 43.76   \u001b[0m | \u001b[0m 0.9351  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.4214  \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.1917  \u001b[0m | \u001b[0m 0.1604  \u001b[0m | \u001b[0m 7.11    \u001b[0m | \u001b[0m 30.56   \u001b[0m | \u001b[0m 93.3    \u001b[0m | \u001b[0m 7.105   \u001b[0m | \u001b[0m 0.4523  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.423   \u001b[0m | \u001b[95m 0.4121  \u001b[0m | \u001b[95m 0.4163  \u001b[0m | \u001b[95m 0.1578  \u001b[0m | \u001b[95m 8.471   \u001b[0m | \u001b[95m 49.04   \u001b[0m | \u001b[95m 81.92   \u001b[0m | \u001b[95m 46.15   \u001b[0m | \u001b[95m 0.8683  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.4168  \u001b[0m | \u001b[0m 0.471   \u001b[0m | \u001b[0m 0.32    \u001b[0m | \u001b[0m 0.03724 \u001b[0m | \u001b[0m 8.769   \u001b[0m | \u001b[0m 28.48   \u001b[0m | \u001b[0m 47.32   \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 0.8645  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.4132  \u001b[0m | \u001b[0m 0.6737  \u001b[0m | \u001b[0m 0.2842  \u001b[0m | \u001b[0m 0.01357 \u001b[0m | \u001b[0m 7.464   \u001b[0m | \u001b[0m 32.54   \u001b[0m | \u001b[0m 65.52   \u001b[0m | \u001b[0m 94.37   \u001b[0m | \u001b[0m 0.8091  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.423   \u001b[0m | \u001b[0m 0.6157  \u001b[0m | \u001b[0m 0.2185  \u001b[0m | \u001b[0m 0.1425  \u001b[0m | \u001b[0m 5.24    \u001b[0m | \u001b[0m 35.0    \u001b[0m | \u001b[0m 70.36   \u001b[0m | \u001b[0m 21.04   \u001b[0m | \u001b[0m 0.4774  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4193  \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.1819  \u001b[0m | \u001b[0m 0.1183  \u001b[0m | \u001b[0m 6.75    \u001b[0m | \u001b[0m 49.48   \u001b[0m | \u001b[0m 19.18   \u001b[0m | \u001b[0m 20.89   \u001b[0m | \u001b[0m 0.4968  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.4171  \u001b[0m | \u001b[0m 0.7919  \u001b[0m | \u001b[0m 0.1266  \u001b[0m | \u001b[0m 0.0986  \u001b[0m | \u001b[0m 5.975   \u001b[0m | \u001b[0m 12.15   \u001b[0m | \u001b[0m 19.93   \u001b[0m | \u001b[0m 65.63   \u001b[0m | \u001b[0m 0.4829  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.4191  \u001b[0m | \u001b[0m 0.5179  \u001b[0m | \u001b[0m 0.1844  \u001b[0m | \u001b[0m 0.166   \u001b[0m | \u001b[0m 5.387   \u001b[0m | \u001b[0m 42.71   \u001b[0m | \u001b[0m 18.65   \u001b[0m | \u001b[0m 97.65   \u001b[0m | \u001b[0m 0.6812  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.4191  \u001b[0m | \u001b[0m 0.9861  \u001b[0m | \u001b[0m 0.3024  \u001b[0m | \u001b[0m 0.1505  \u001b[0m | \u001b[0m 5.156   \u001b[0m | \u001b[0m 17.73   \u001b[0m | \u001b[0m 20.82   \u001b[0m | \u001b[0m 29.61   \u001b[0m | \u001b[0m 0.4712  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.4228  \u001b[0m | \u001b[0m 0.7347  \u001b[0m | \u001b[0m 0.2391  \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 5.098   \u001b[0m | \u001b[0m 5.109   \u001b[0m | \u001b[0m 99.29   \u001b[0m | \u001b[0m 23.31   \u001b[0m | \u001b[0m 0.4224  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.4218  \u001b[0m | \u001b[0m 0.8888  \u001b[0m | \u001b[0m 0.4904  \u001b[0m | \u001b[0m 0.09733 \u001b[0m | \u001b[0m 5.046   \u001b[0m | \u001b[0m 47.79   \u001b[0m | \u001b[0m 99.72   \u001b[0m | \u001b[0m 39.87   \u001b[0m | \u001b[0m 0.9458  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.421   \u001b[0m | \u001b[0m 0.7503  \u001b[0m | \u001b[0m 0.07817 \u001b[0m | \u001b[0m 0.1567  \u001b[0m | \u001b[0m 5.038   \u001b[0m | \u001b[0m 49.19   \u001b[0m | \u001b[0m 50.67   \u001b[0m | \u001b[0m 55.19   \u001b[0m | \u001b[0m 0.7147  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.4228  \u001b[0m | \u001b[0m 0.7988  \u001b[0m | \u001b[0m 0.1585  \u001b[0m | \u001b[0m 0.1027  \u001b[0m | \u001b[0m 5.497   \u001b[0m | \u001b[0m 49.46   \u001b[0m | \u001b[0m 98.5    \u001b[0m | \u001b[0m 0.4736  \u001b[0m | \u001b[0m 0.4071  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.4226  \u001b[0m | \u001b[0m 0.4137  \u001b[0m | \u001b[0m 0.1919  \u001b[0m | \u001b[0m 0.1532  \u001b[0m | \u001b[0m 5.005   \u001b[0m | \u001b[0m 9.044   \u001b[0m | \u001b[0m 97.96   \u001b[0m | \u001b[0m 0.7933  \u001b[0m | \u001b[0m 0.8389  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.4227  \u001b[0m | \u001b[0m 0.601   \u001b[0m | \u001b[0m 0.4254  \u001b[0m | \u001b[0m 0.09834 \u001b[0m | \u001b[0m 5.003   \u001b[0m | \u001b[0m 49.5    \u001b[0m | \u001b[0m 99.49   \u001b[0m | \u001b[0m 6.535   \u001b[0m | \u001b[0m 0.4415  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.4221  \u001b[0m | \u001b[0m 0.4095  \u001b[0m | \u001b[0m 0.3623  \u001b[0m | \u001b[0m 0.1729  \u001b[0m | \u001b[0m 5.042   \u001b[0m | \u001b[0m 48.53   \u001b[0m | \u001b[0m 96.62   \u001b[0m | \u001b[0m 37.21   \u001b[0m | \u001b[0m 0.6071  \u001b[0m |\n",
      "| \u001b[95m 18      \u001b[0m | \u001b[95m 0.4234  \u001b[0m | \u001b[95m 0.4294  \u001b[0m | \u001b[95m 0.3934  \u001b[0m | \u001b[95m 0.1266  \u001b[0m | \u001b[95m 5.547   \u001b[0m | \u001b[95m 5.037   \u001b[0m | \u001b[95m 99.95   \u001b[0m | \u001b[95m 3.738   \u001b[0m | \u001b[95m 0.8392  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.4222  \u001b[0m | \u001b[0m 0.7148  \u001b[0m | \u001b[0m 0.4857  \u001b[0m | \u001b[0m 0.05311 \u001b[0m | \u001b[0m 5.099   \u001b[0m | \u001b[0m 31.22   \u001b[0m | \u001b[0m 99.33   \u001b[0m | \u001b[0m 12.38   \u001b[0m | \u001b[0m 0.9643  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.4229  \u001b[0m | \u001b[0m 0.7074  \u001b[0m | \u001b[0m 0.3886  \u001b[0m | \u001b[0m 0.1241  \u001b[0m | \u001b[0m 5.315   \u001b[0m | \u001b[0m 5.854   \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 4.339   \u001b[0m | \u001b[0m 0.9836  \u001b[0m |\n",
      "=========================================================================================================================\n",
      "Time taken for hyper-parameter optimization:834.095696926117\n"
     ]
    }
   ],
   "source": [
    "lgc = bayes_opt_f1(x_train,y_train,x_val,y_val, init_round=10, opt_round=10, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "mjn7ySVY_jgn",
    "outputId": "e2270547-d6ba-414d-91f9-d9582bf538aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score(micro): 0.28717156902067487\n",
      "F1 Score(macro): 0.42390294016926766\n",
      "Accuracy: 0.42390294016926766\n",
      "Mean Squared Error: 1.7868254977740814\n"
     ]
    }
   ],
   "source": [
    "lgc.fit(x_train,y_train)\n",
    "y_val_pred = lgc.predict(x_val)\n",
    "print(\"F1 Score(micro):\",f1_score(y_val,y_val_pred,average='macro'))\n",
    "print(\"F1 Score(macro):\",f1_score(y_val,y_val_pred,average='micro'))\n",
    "print(\"Accuracy:\",accuracy_score(y_val,y_val_pred))\n",
    "print(\"Mean Squared Error:\",mean_squared_error(y_val,y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q91VhZ13A4ZY"
   },
   "outputs": [],
   "source": [
    "def bayes_opt_mse(x_train,y_train,x_val,y_val, init_round=15, opt_round=15, random_seed=SEED):\n",
    "    t1 = time.time()\n",
    "    \n",
    "\n",
    "        \n",
    "    def lgb_eval(learning_rate,n_estimators,max_depth,min_child_weight,gamma,subsample,colsample_bytree,reg_alpha):\n",
    "        \n",
    "        lgr = LGBMRegressor(learning_rate=learning_rate,n_estimators=int(n_estimators)\n",
    "                            ,max_depth=int(max_depth),min_child_weight=min_child_weight,\n",
    "                             gamma=gamma,subsample=subsample,colsample_bytree=colsample_bytree,\n",
    "                             reg_aplha=reg_alpha)\n",
    "        lgr.fit(x_train,y_train)\n",
    "        y_val_pred = lgr.predict(x_val)\n",
    "        return((-1)*mean_squared_error(y_val,y_val_pred))\n",
    "        \n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.01,0.2),\n",
    "                                            'n_estimators': (10,100),\n",
    "                                            'max_depth': (5, 8.99),\n",
    "                                            'min_child_weight': (5, 50),\n",
    "                                            'gamma':(0,0.5),\n",
    "                                            'subsample': (0.4, 1.0),\n",
    "                                            'colsample_bytree' :(0.4, 1.0),\n",
    "                                            'reg_alpha':(0.001,100)\n",
    "                                            }, \n",
    "                                               random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "\n",
    "    \n",
    "    # return best parameters\n",
    "    targets = np.array([d['target'] for d in lgbBO.res])\n",
    "    opt_params = lgbBO.res[np.argmax(targets)]['params']\n",
    "    opt_params['max_depth'] = int(opt_params['max_depth'])\n",
    "    opt_params['n_estimators'] = int(opt_params['n_estimators'])\n",
    "    lgr = LGBMRegressor(**opt_params)\n",
    "    print(\"Time taken for hyper-parameter optimization:\" + str(time.time()-t1) )\n",
    "    return(lgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "hr8pdNybBfNY",
    "outputId": "1b0404ed-c684-4baf-c8ee-b809a5d9f3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | n_esti... | reg_alpha | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.223   \u001b[0m | \u001b[0m 0.7293  \u001b[0m | \u001b[0m 0.3576  \u001b[0m | \u001b[0m 0.1245  \u001b[0m | \u001b[0m 7.174   \u001b[0m | \u001b[0m 24.06   \u001b[0m | \u001b[0m 68.13   \u001b[0m | \u001b[0m 43.76   \u001b[0m | \u001b[0m 0.9351  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-1.226   \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.1917  \u001b[0m | \u001b[0m 0.1604  \u001b[0m | \u001b[0m 7.11    \u001b[0m | \u001b[0m 30.56   \u001b[0m | \u001b[0m 93.3    \u001b[0m | \u001b[0m 7.105   \u001b[0m | \u001b[0m 0.4523  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-1.224   \u001b[0m | \u001b[0m 0.4121  \u001b[0m | \u001b[0m 0.4163  \u001b[0m | \u001b[0m 0.1578  \u001b[0m | \u001b[0m 8.471   \u001b[0m | \u001b[0m 49.04   \u001b[0m | \u001b[0m 81.92   \u001b[0m | \u001b[0m 46.15   \u001b[0m | \u001b[0m 0.8683  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-1.231   \u001b[0m | \u001b[0m 0.471   \u001b[0m | \u001b[0m 0.32    \u001b[0m | \u001b[0m 0.03724 \u001b[0m | \u001b[0m 8.769   \u001b[0m | \u001b[0m 28.48   \u001b[0m | \u001b[0m 47.32   \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 0.8645  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.25    \u001b[0m | \u001b[0m 0.6737  \u001b[0m | \u001b[0m 0.2842  \u001b[0m | \u001b[0m 0.01357 \u001b[0m | \u001b[0m 7.464   \u001b[0m | \u001b[0m 32.54   \u001b[0m | \u001b[0m 65.52   \u001b[0m | \u001b[0m 94.37   \u001b[0m | \u001b[0m 0.8091  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-1.223   \u001b[0m | \u001b[95m 0.6157  \u001b[0m | \u001b[95m 0.2185  \u001b[0m | \u001b[95m 0.1425  \u001b[0m | \u001b[95m 5.24    \u001b[0m | \u001b[95m 35.0    \u001b[0m | \u001b[95m 70.36   \u001b[0m | \u001b[95m 21.04   \u001b[0m | \u001b[95m 0.4774  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-1.225   \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.1819  \u001b[0m | \u001b[0m 0.1183  \u001b[0m | \u001b[0m 6.75    \u001b[0m | \u001b[0m 49.48   \u001b[0m | \u001b[0m 19.18   \u001b[0m | \u001b[0m 20.89   \u001b[0m | \u001b[0m 0.4968  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.225   \u001b[0m | \u001b[0m 0.7919  \u001b[0m | \u001b[0m 0.1266  \u001b[0m | \u001b[0m 0.0986  \u001b[0m | \u001b[0m 5.975   \u001b[0m | \u001b[0m 12.15   \u001b[0m | \u001b[0m 19.93   \u001b[0m | \u001b[0m 65.63   \u001b[0m | \u001b[0m 0.4829  \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-1.223   \u001b[0m | \u001b[0m 0.5179  \u001b[0m | \u001b[0m 0.1844  \u001b[0m | \u001b[0m 0.166   \u001b[0m | \u001b[0m 5.387   \u001b[0m | \u001b[0m 42.71   \u001b[0m | \u001b[0m 18.65   \u001b[0m | \u001b[0m 97.65   \u001b[0m | \u001b[0m 0.6812  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-1.223   \u001b[0m | \u001b[95m 0.9861  \u001b[0m | \u001b[95m 0.3024  \u001b[0m | \u001b[95m 0.1505  \u001b[0m | \u001b[95m 5.156   \u001b[0m | \u001b[95m 17.73   \u001b[0m | \u001b[95m 20.82   \u001b[0m | \u001b[95m 29.61   \u001b[0m | \u001b[95m 0.4712  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.224   \u001b[0m | \u001b[0m 0.7347  \u001b[0m | \u001b[0m 0.2391  \u001b[0m | \u001b[0m 0.1395  \u001b[0m | \u001b[0m 5.098   \u001b[0m | \u001b[0m 5.109   \u001b[0m | \u001b[0m 99.29   \u001b[0m | \u001b[0m 23.31   \u001b[0m | \u001b[0m 0.4224  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-1.229   \u001b[0m | \u001b[0m 0.5837  \u001b[0m | \u001b[0m 0.3462  \u001b[0m | \u001b[0m 0.1503  \u001b[0m | \u001b[0m 5.238   \u001b[0m | \u001b[0m 48.92   \u001b[0m | \u001b[0m 11.27   \u001b[0m | \u001b[0m 63.84   \u001b[0m | \u001b[0m 0.6387  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.288   \u001b[0m | \u001b[0m 0.6925  \u001b[0m | \u001b[0m 0.4604  \u001b[0m | \u001b[0m 0.04185 \u001b[0m | \u001b[0m 5.537   \u001b[0m | \u001b[0m 6.354   \u001b[0m | \u001b[0m 10.6    \u001b[0m | \u001b[0m 0.952   \u001b[0m | \u001b[0m 0.4674  \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m-1.221   \u001b[0m | \u001b[95m 0.6571  \u001b[0m | \u001b[95m 0.4236  \u001b[0m | \u001b[95m 0.0573  \u001b[0m | \u001b[95m 5.679   \u001b[0m | \u001b[95m 6.322   \u001b[0m | \u001b[95m 99.03   \u001b[0m | \u001b[95m 98.62   \u001b[0m | \u001b[95m 0.5616  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-1.354   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 7.366e-1\u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[95m 16      \u001b[0m | \u001b[95m-1.221   \u001b[0m | \u001b[95m 0.4     \u001b[0m | \u001b[95m 0.2608  \u001b[0m | \u001b[95m 0.1111  \u001b[0m | \u001b[95m 5.0     \u001b[0m | \u001b[95m 50.0    \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 0.4     \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.225   \u001b[0m | \u001b[0m 0.5684  \u001b[0m | \u001b[0m 0.4004  \u001b[0m | \u001b[0m 0.1639  \u001b[0m | \u001b[0m 5.249   \u001b[0m | \u001b[0m 49.38   \u001b[0m | \u001b[0m 97.97   \u001b[0m | \u001b[0m 1.483   \u001b[0m | \u001b[0m 0.9341  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-1.225   \u001b[0m | \u001b[0m 0.6588  \u001b[0m | \u001b[0m 0.2616  \u001b[0m | \u001b[0m 0.1446  \u001b[0m | \u001b[0m 5.597   \u001b[0m | \u001b[0m 27.1    \u001b[0m | \u001b[0m 99.98   \u001b[0m | \u001b[0m 66.74   \u001b[0m | \u001b[0m 0.5682  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.224   \u001b[0m | \u001b[0m 0.7599  \u001b[0m | \u001b[0m 0.3073  \u001b[0m | \u001b[0m 0.1414  \u001b[0m | \u001b[0m 5.108   \u001b[0m | \u001b[0m 49.81   \u001b[0m | \u001b[0m 45.3    \u001b[0m | \u001b[0m 76.1    \u001b[0m | \u001b[0m 0.8522  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-1.224   \u001b[0m | \u001b[0m 0.6243  \u001b[0m | \u001b[0m 0.1449  \u001b[0m | \u001b[0m 0.1716  \u001b[0m | \u001b[0m 5.098   \u001b[0m | \u001b[0m 5.295   \u001b[0m | \u001b[0m 47.75   \u001b[0m | \u001b[0m 56.4    \u001b[0m | \u001b[0m 0.6656  \u001b[0m |\n",
      "=========================================================================================================================\n",
      "Time taken for hyper-parameter optimization:253.8691165447235\n"
     ]
    }
   ],
   "source": [
    "lgr = bayes_opt_mse(x_train,y_train,x_val,y_val, init_round=10, opt_round=10, random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y-jXfVh0CPxD",
    "outputId": "3e2d0d73-5d28-4829-b455-b8f4f7aa3e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.7868254977740814\n"
     ]
    }
   ],
   "source": [
    "lgr.fit(x_train,y_train)\n",
    "y_val_pred = lgc.predict(x_val)\n",
    "print(\"Mean Squared Error:\",mean_squared_error(y_val,y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "MFf3WcMbCYLy",
    "outputId": "92668f68-2d07-4e3a-d03b-65291c54d15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score(micro): 0.28717156902067487\n",
      "F1 Score(macro): 0.42390294016926766\n",
      "Accuracy: 0.42390294016926766\n"
     ]
    }
   ],
   "source": [
    "nv = np.vectorize(nearest_integer)\n",
    "y_val_pred = nv(y_val_pred)\n",
    "print(\"F1 Score(micro):\",f1_score(y_val,y_val_pred,average='macro'))\n",
    "print(\"F1 Score(macro):\",f1_score(y_val,y_val_pred,average='micro'))\n",
    "print(\"Accuracy:\",accuracy_score(y_val,y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htEamdbfH7ja"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task1_modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
